- trzeba przetworzyć dane "Myślałem, żeby podzielić sygnał na kawałki, z nich wygenerować obrazy i
potworzyć pary: obraz-typ arytmii czyli input-destination dla sieci
neuronowej. Obraz to może być wykreślony przebieg albo reprezentacja 2-wymiarowa
sygnału (spektrogram, falki itp.)"

- należy pamiętać że uzywany jest tylko jeden sygnał z dwóch, "klasyfikacja na podstawie 
jednego sygnału"

- stworzyć jakieś bardziej uogólnione wczytywanie wyszukiwanie QRS, te co teraz jest 
bazuje na bazie MIT-BIH

- opisać który program co robi

- trzeba podzielić zmienną X_train na kilka zmiennych, jedna jest za duża

- przetworzyć również drugi sygnał i zapisać, pamiętać by był razem zapisany z tym pierwszym

- stworzyć program który będzie wyświetlał 2 wykresy z każdego typu uderzeń

- sprawdzić czy na pewno jest ten drugi sygnał przesunięty wszędzie o tyle samo
(chyba na stronie bazy mit-bih było że przesunięcie jest stałe i jaką ma wartość)

##########################################################
- ustawić pseudo losowość by wynik był powtarzalny (jest prawie powtarzalny) różnice są pewnie przez
GPU, podobno on nadaje jakieś losowe liczby również
- zapisać razem wszystkie dane a później je podzielić albo podzielić w sposób losowy

- sprawdzić zmienne y_train i y_test

- stworzyć zwykłą cnn głęboką sieć neuronową by sprawdzić jak działa na moich danych,
shape początkowy -> X_train (60000, 28, 28) <- macierz macierzy 28x28
shape początkowy -> y_train (60000,) <- lista cyfr 0..9 (można to w keras łatwo zamienić na wektor)

- Może wczytać z każdego typu po 2k zdjęć do treningu i później 0.5k do testów, a wyjście 
jako macierz wektorów (0 0 0 1 0 0)

- dać wszystko na githuba to można bedzie z niego wczytać do colaba

- zmniejszyć zdjęcia i zmienić na czarnobiałe, jest ich tyle że lepiej jak by 
były prostrze, np 470x470 jak było w tym wyjściowym wektorze (jest 496x369),
zmieniłem na 220x220 dalej rgb

- zapisać wszystkie obrazy z csv typów uderzeń serca

- użyć multiprocessing by przyśpieszyć wykonanie tworzenia zdjęć spektrogramu i zmiejszyć
wielkość zdjęcia (udało się zejść z 10h do 1h 30 min)

- utworzyć oddzielny folder na stare programy które są już nie potrzebne

- odrzucić typy uderzeń których jest za mało

- przeanalizować spektrogram, coś jest nie tak

- pogrupować przedziały na typy i zapisać do plików mających nazwy typów

- zamiast używania pętli i oznaczania wszystkiego co nie jest adnotacją peaks jako '0'
użyć funkcji filter([function | expression], [collection]) <- zwraca generator nie listę

- można stwożyć słownik (jeśli będzie potrzebny) za pomocą dict(zip(pierwsza_lista, druga_lista))

- poczytać o arytmii, czy na pewno w taki sposób można wyszukać wadę, tzn. czy można 
zrobić to patrząc tylko na zbiór wycinków, nie przetwarzając sygnału w całości

- uwzglęndić tylko te wzniesienia w adnotacjach które nie są błędne (jest chyba kilka adnotacji
które opisują błędy a nie wzniesienia)

- poprawienie dokładności preprocesingu, by lepiej znajdował wzniesienia,
najlepiej by wykożystywał adnotacje a nie sam szukał, chociaż takie wyszukiwanie też
się przyda np. jak będzie się chciało wykożystać sygnał z innej bazy, bez adnotacji

- może zrobić bez rozróżnienia dobrych wzniesień bo przecież i te złe wzniesienia są ważne,
a dopiero na samym końcu wywalić te co są błędne (błęd aparatury)

- zamiana ciągłego kodu na tak z użyciem funkcji, lepiej zmieniać i lepiej wygląda

- stworzenie programu który pokazuje stworzone pliki .csv jako obrazy,
widać na nich bardzo szybko czy dobrze zostały zebrane wzniesienia