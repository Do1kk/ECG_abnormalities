- trzeba przetworzyć dane "Myślałem, żeby podzielić sygnał na kawałki, z nich wygenerować obrazy i
potworzyć pary: obraz-typ arytmii czyli input-destination dla sieci
neuronowej. Obraz to może być wykreślony przebieg albo reprezentacja 2-wymiarowa
sygnału (spektrogram, falki itp.)"

- może testy jakieś wprowadzić, tak jak jest w książce na przykład

- użyć również drugiego sygnału (może połączyć razem z pierwszym i zrobić jeden obraz?) "klasyfikacja na podstawie 
dwóch sygnałów"

- stworzyć jakieś bardziej uogólnione wczytywanie wyszukiwanie QRS, te co teraz jest 
bazuje na bazie MIT-BIH

- przetworzyć również drugi sygnał i zapisać, pamiętać by był razem zapisany z tym pierwszym

- sprawdzić czy na pewno jest ten drugi sygnał przesunięty wszędzie o tyle samo
(chyba na stronie bazy mit-bih było że przesunięcie jest stałe i jaką ma wartość)

- uzyc "Batch Norm" na kilku warstwach, przyspieszy uczenie ale da tez troche zakłuceń(to dobrze bo bedzie
wieksze val_acc)

- może jak by te zdjęcia wypłaszczył (zamienił np 10x10x3 na wektor o długości 300) to by zajmowały mniej
i szybciej się sieć uczyła?

- stworzyć optymalizator do kodu uczącego (keras itp) by zoptymalizować hiperparametry,
może rzeby dobierał jakieś i sprawdzał wyniki, może randomowo i zapisywał te najlepsze wyniki

- używać więcej funkcji wbudowanych w np, (np.sum() ... np.add() itp)

- importowanie zmiennyc/funkcji z innych plików by nie powtarzać kodu

- # Nowy podział na grupy:
# N <- non-ectopic,
# S <- supraventricular ectopic,
# V <- ventricular ectopic,
# F <- fusion beats,
# Q <- Unknown beats.
# co ciekawe zostały jeszcze typy "!, P, p".....

##########################################################
- brać zdjęcia wybiórczo (są brane z początku np dla typu N jest brane 2540 zdjęc z początku wszystkich 74tys),
teraz jest mięszane podczas podziału w image_split.py
- w png_spec.py ustawic by robił wszystkie typy zdjęć w pętli, jest mały problem z multiprocessingem

- przesunąć wycięte przedziały, dlatego że bicie serca nie jest wyśrodkowane do głównego peak (lewa 
strona jest krótsza a prawa dłuższa), w magisterce można podać obliczenia czemu taka długość przedziału
i czemu takie przesunięcie

- może dać opis do funkcji typu ''' dsfsdf '''

- może też dała by coś normalizacja danych pikseli (że podzielić wszytkie przez 255) ale wtedy trzeba zmienić 
typ danych z uint8 na inny (taki co przechowuje liczby z zakresu [0, 1], dodatkowo liczby zmienno przecinkowe 
są lepiej liczone przez GPU

- liczba batch (mini-batch) powinna być wielokrotnością 2, np 128

- zrobić w kers generatory, może szybciej będzie to działać, mniejsze użycie ramu,
można wtedy kożystać ze zdjęć a nie z spakowanego pliku numpy na podstwaie zdjęć, jest szybciej
i zostawia się wszystko bibliotece keras i tensorflow

- należy pamiętać że uzywany jest tylko jeden sygnał z dwóch, "klasyfikacja na podstawie 
jednego sygnału"

- opisać który program co robi, 

- stworzyć program który będzie wyświetlał 10 wykresów z każdego typu uderzeń

- ustawić pseudo losowość by wynik był powtarzalny (jest prawie powtarzalny) różnice są pewnie przez
GPU, podobno on nadaje jakieś losowe liczby również

- zapisać razem wszystkie dane a później je podzielić albo podzielić w sposób losowy

- sprawdzić zmienne y_train i y_test

- stworzyć zwykłą cnn głęboką sieć neuronową by sprawdzić jak działa na moich danych,
shape początkowy -> X_train (60000, 28, 28) <- macierz macierzy 28x28
shape początkowy -> y_train (60000,) <- lista cyfr 0..9 (można to w keras łatwo zamienić na wektor)

- Może wczytać z każdego typu po 2k zdjęć do treningu i później 0.5k do testów, a wyjście 
jako macierz wektorów (0 0 0 1 0 0)

- dać wszystko na githuba to można bedzie z niego wczytać do colaba

- zmniejszyć zdjęcia i zmienić na czarnobiałe, jest ich tyle że lepiej jak by 
były prostrze, np 470x470 jak było w tym wyjściowym wektorze (jest 496x369),
zmieniłem na 220x220 dalej rgb

- zapisać wszystkie obrazy z csv typów uderzeń serca

- użyć multiprocessing by przyśpieszyć wykonanie tworzenia zdjęć spektrogramu i zmiejszyć
wielkość zdjęcia (udało się zejść z 10h do 1h 30 min)

- utworzyć oddzielny folder na stare programy które są już nie potrzebne

- odrzucić typy uderzeń których jest za mało

- przeanalizować spektrogram, coś jest nie tak

- pogrupować przedziały na typy i zapisać do plików mających nazwy typów

- zamiast używania pętli i oznaczania wszystkiego co nie jest adnotacją peaks jako '0'
użyć funkcji filter([function | expression], [collection]) <- zwraca generator nie listę

- można stwożyć słownik (jeśli będzie potrzebny) za pomocą dict(zip(pierwsza_lista, druga_lista))

- poczytać o arytmii, czy na pewno w taki sposób można wyszukać wadę, tzn. czy można 
zrobić to patrząc tylko na zbiór wycinków, nie przetwarzając sygnału w całości

- uwzglęndić tylko te wzniesienia w adnotacjach które nie są błędne (jest chyba kilka adnotacji
które opisują błędy a nie wzniesienia)

- poprawienie dokładności preprocesingu, by lepiej znajdował wzniesienia,
najlepiej by wykożystywał adnotacje a nie sam szukał, chociaż takie wyszukiwanie też
się przyda np. jak będzie się chciało wykożystać sygnał z innej bazy, bez adnotacji

- może zrobić bez rozróżnienia dobrych wzniesień bo przecież i te złe wzniesienia są ważne,
a dopiero na samym końcu wywalić te co są błędne (błęd aparatury)

- zamiana ciągłego kodu na tak z użyciem funkcji, lepiej zmieniać i lepiej wygląda

- stworzenie programu który pokazuje stworzone pliki .csv jako obrazy,
widać na nich bardzo szybko czy dobrze zostały zebrane wzniesienia