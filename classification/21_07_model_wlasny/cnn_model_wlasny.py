# -*- coding: utf-8 -*-
"""CNN_model_wlasny.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12yNecLO8L4tB0pjsgd4fJEYqkRDSR4LQ
"""

# Commented out IPython magic to ensure Python compatibility.
# Użycie zapisanych na dysku zdjęć.
!unzip -q '/content/drive/My Drive/train_val_test.zip' -d '/content'
# Load the TensorBoard notebook extension. #%reload_ext tensorboard
# %load_ext tensorboard

from numpy.random import seed
seed(1)
from tensorflow.random import set_seed
set_seed(2)

import os
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard
from tensorflow.keras.optimizers import Adamax, Nadam, Ftrl, Adadelta
from tensorflow.keras.regularizers import l1_l2

BATCH_SIZE = 32
EPOCHS = 50
INITIAL_EPOCH = 0
IMG_HEIGHT, IMG_WIDTH, CHANNELS = 220, 220, 3

dir_path = "/content/train_val_test"
root = "/content/drive/My Drive/CNN_model_wlasny/" 

train_gen = ImageDataGenerator(rescale=1.0/255.)
val_gen = ImageDataGenerator(rescale=1.0/255.)
test_gen = ImageDataGenerator(rescale=1.0/255.)

train_generator = train_gen.flow_from_directory(
    dir_path + '/train',
    batch_size=BATCH_SIZE,
    shuffle=True,
    color_mode="rgb",
    class_mode='categorical',
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)
validation_generator = val_gen.flow_from_directory(
    dir_path + '/val',
    batch_size=BATCH_SIZE,
    shuffle=True,
    color_mode="rgb",
    class_mode='categorical',
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)
test_generator = test_gen.flow_from_directory(
    dir_path + '/test',
    batch_size=1,
    shuffle=False,
    color_mode="rgb",
    class_mode='categorical',
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)

STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size

my_callbacks = [
    ModelCheckpoint(filepath=root + "model.{epoch:02d}.h5",
                    monitor="val_accuracy",
                    mode='max',
                    save_best_only=True,
                    save_freq="epoch",),
    TensorBoard(log_dir=root + "logs",
                write_images=False,
                histogram_freq=1,
                embeddings_freq=2),
]

# Użycie zapisanego na dysku modelu.
files = [f for f in sorted(os.listdir(root))]
model_file = files[-1]
INITIAL_EPOCH = int(model_file.split('.')[1])
EPOCHS += INITIAL_EPOCH
    
model = load_model(root + model_file)
print(f"Wczytanie pliku modelu: {model_file}, z ilością EPOCH: {INITIAL_EPOCH}")

# # Tworzenie modelu. Jeśli model jest wczytywany to nie tworzyć bo to go usunie.
# model = Sequential()
# model.add(Conv2D(8, kernel_size=3, activation="relu", 
#                  input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), 
#                  data_format="channels_last"))
# model.add(Conv2D(8, kernel_size=3, activation="relu"))
# model.add(BatchNormalization())
# model.add(Conv2D(8, kernel_size=3, activation="relu"))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(16, kernel_size=3, activation="relu"))
# model.add(Conv2D(16, kernel_size=3, activation="relu"))
# model.add(BatchNormalization())
# model.add(Conv2D(32, kernel_size=3, activation="relu"))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(32, kernel_size=3, activation="relu"))
# model.add(Conv2D(64, kernel_size=3, activation="relu"))
# model.add(BatchNormalization())
# model.add(Conv2D(64, kernel_size=3, activation="relu"))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(BatchNormalization())
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(MaxPooling2D(pool_size=(2, 2)))
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(BatchNormalization())
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(Flatten())
# model.add(Dense(5, activation="softmax"))
# model.compile(optimizer=Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), 
#               loss="categorical_crossentropy", 
#               metrics=["accuracy"])

# model.summary()

# Tworzenie modelu. Jeśli model jest wczytywany to nie tworzyć bo to go usunie.
model = Sequential()
model.add(Conv2D(32, kernel_size=3, activation="relu", 
                 input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS), 
                 data_format="channels_last"))
model.add(Conv2D(32, kernel_size=3, activation="relu"))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size=3, activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(32, kernel_size=3, activation="relu"))
model.add(Conv2D(32, kernel_size=3, activation="relu"))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size=3, activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(BatchNormalization())
model.add(Conv2D(64, kernel_size=3, activation="relu"))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=3, activation="relu"))
model.add(Conv2D(128, kernel_size=3, activation="relu"))
model.add(BatchNormalization())
model.add(Conv2D(128, kernel_size=3, activation="relu"))
# model.add(Conv2D(128, kernel_size=3, activation="relu"))
model.add(Flatten())
model.add(Dense(256, activation="relu"))
model.add(Dropout(0.2))
model.add(Dense(128, activation="relu"))
model.add(Dropout(0.2))
model.add(Dense(64, activation="relu"))
# model.add(Dropout(0.2))
model.add(Dense(5, activation="softmax"))
model.compile(optimizer=Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), 
              loss="categorical_crossentropy", 
              metrics=["accuracy"])

model.summary()

model.summary()

# Commented out IPython magic to ensure Python compatibility.
# Włączenie tensorboard.
# %tensorboard --logdir "/content/drive/My Drive/CNN_model_wlasny/logs"

history = model.fit(train_generator, 
                    steps_per_epoch=STEP_SIZE_TRAIN,
                    validation_data=validation_generator,
                    validation_steps=STEP_SIZE_VALID,
                    epochs=EPOCHS, shuffle=True,
                    callbacks=my_callbacks,
                    initial_epoch=INITIAL_EPOCH,
                    use_multiprocessing=False, verbose=1)

# Sprawdzenie poprawności na danych testowych
test_generator.reset()
STEP_SIZE_TEST = test_generator.n // test_generator.batch_size 
scores = model.evaluate(test_generator, 
                        steps=STEP_SIZE_TEST,
                        verbose=0)
print(model.metrics_names[0] + " = ", scores[0])
print(model.metrics_names[1] + " = ", scores[1])

test_generator.reset()
probabilities = model.predict(test_generator, 
                              steps=STEP_SIZE_TEST,
                              verbose=0)
y_test = probabilities.argmax(axis=1)
y_pred = test_generator.classes
target_names = sorted(os.listdir(dir_path + "/test"))
print(classification_report(y_test, y_pred, target_names=target_names))

# Zapisanie pliku .csv porównując typ i predykcję, jest podane zdjęcie więc łatwo będzie sprawdzić co z nim nie tak
test_generator.reset()
probabilities = model.predict(test_generator, 
                              steps=STEP_SIZE_TEST,
                              verbose=1)
predicted_class_indices = np.argmax(probabilities, axis=1)
labels = (train_generator.class_indices)
labels = dict((v, k) for k, v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

filenames = test_generator.filenames
results=pd.DataFrame({"Filename": filenames,
                      "Predictions": predictions,})
results.to_csv("results.csv", sep=';', index=False)