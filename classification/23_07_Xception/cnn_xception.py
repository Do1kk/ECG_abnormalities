# -*- coding: utf-8 -*-
"""CNN_xception.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vp3z1tKjMQOW44QccSRxffFlZbErKXhB
"""

# Commented out IPython magic to ensure Python compatibility.
# Użycie zapisanych na dysku zdjęć.
!unzip -q '/content/drive/My Drive/train_val_test.zip' -d '/content'
# Load the TensorBoard notebook extension. #%reload_ext tensorboard
# %load_ext tensorboard

from numpy.random import seed
seed(1)
from tensorflow.random import set_seed
set_seed(2)

import os
import numpy as np
import pandas as pd
from sklearn.metrics import classification_report

from tensorflow.keras import applications
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model, Model
from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping
from tensorflow.keras.optimizers import Adamax, Nadam, Adadelta
from tensorflow.keras.regularizers import l1_l2

BATCH_SIZE = 32
EPOCHS = 50
INITIAL_EPOCH = 0
IMG_HEIGHT, IMG_WIDTH, CHANNELS = 220, 220, 3

dir_path = "/content/train_val_test"
root = "/content/drive/My Drive/CNN_xception/" 

train_gen = ImageDataGenerator(rescale=1.0/255.)
val_gen = ImageDataGenerator(rescale=1.0/255.)
test_gen = ImageDataGenerator(rescale=1.0/255.)

train_generator = train_gen.flow_from_directory(
    dir_path + "/train",
    batch_size=BATCH_SIZE,
    shuffle=True,
    color_mode="rgb",
    class_mode="categorical",
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)
validation_generator = val_gen.flow_from_directory(
    dir_path + "/val",
    batch_size=BATCH_SIZE,
    shuffle=True,
    color_mode="rgb",
    class_mode="categorical",
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)
test_generator = test_gen.flow_from_directory(
    dir_path + "/test",
    batch_size=1,
    shuffle=False,
    color_mode="rgb",
    class_mode="categorical",
    target_size=(IMG_HEIGHT, IMG_WIDTH),
    seed=2)

STEP_SIZE_TRAIN = train_generator.n // train_generator.batch_size
STEP_SIZE_VALID = validation_generator.n // validation_generator.batch_size

my_callbacks = [
    ModelCheckpoint(filepath=root + "model.{epoch:02d}.h5",
                    monitor="val_accuracy",
                    mode="max",
                    save_best_only=True,
                    save_freq="epoch",),
    EarlyStopping(monitor="aac",
                  patience=5,),
    TensorBoard(log_dir=root + "logs",
                write_images=False,
                histogram_freq=1,
                embeddings_freq=2,),
]

# Użycie zapisanego na dysku modelu.
files = [f for f in sorted(os.listdir(root))]
model_file = files[-1]
INITIAL_EPOCH = int(model_file.split('.')[1])
EPOCHS += INITIAL_EPOCH
    
model = load_model(root + model_file)
print(f"Wczytanie pliku modelu: {model_file}, z ilością EPOCH: {INITIAL_EPOCH}")

# Wczytanie modelu, ustawienie wag.
base_model = applications.Xception(weights="imagenet", 
                                   include_top=False, 
                                   pooling="avg",
                                   input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))
# Zamrożenie części warstw by kożystać z wytrenowanych już wag.
base_model.layers[:120].trainable = False # Ciekawe czy to zadziała.
# Dodanie warstwy do wczytanego modelu, uwzględnienie liczbę klas
x = base_model.output
predictions = Dense(5, activation="softmax")(x)
model = Model(inputs = base_model.input, outputs = predictions)

model.compile(optimizer=Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), 
              loss="categorical_crossentropy", 
              metrics=["accuracy"])
model.summary()

# Wczytanie modelu ale bez ostatnich warstw by je podmienić
model = applications.Xception(weights=None, 
                                   include_top=True, 
                                   pooling="avg",
                                   classes=5,
                                   classifier_activation="softmax",
                                   input_shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))

model.compile(optimizer=Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07), 
              loss="categorical_crossentropy", 
              metrics=["accuracy"])
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# Włączenie tensorboard.
# %tensorboard --logdir "/content/drive/My Drive/CNN_xception/logs"

history = model.fit(train_generator, 
                    steps_per_epoch=STEP_SIZE_TRAIN,
                    validation_data=validation_generator,
                    validation_steps=STEP_SIZE_VALID,
                    epochs=EPOCHS, shuffle=True,
                    callbacks=my_callbacks,
                    initial_epoch=INITIAL_EPOCH,
                    use_multiprocessing=False, verbose=1)

# Sprawdzenie poprawności na danych testowych
test_generator.reset()
STEP_SIZE_TEST = test_generator.n // test_generator.batch_size 
scores = model.evaluate(test_generator, 
                        steps=STEP_SIZE_TEST,
                        verbose=0)
print(model.metrics_names[0] + '=', scores[0])
print(model.metrics_names[1] + '=', scores[1])

test_generator.reset()
probabilities = model.predict(test_generator, 
                              steps=STEP_SIZE_TEST,
                              verbose=0)
y_test = probabilities.argmax(axis=1)
y_pred = test_generator.classes
target_names = sorted(os.listdir(dir_path + "/test"))
print(classification_report(y_test, y_pred, target_names=target_names))

# Trenowanie na danych ze zbioru walidacyjnego.
model.fit(validation_generator, 
          steps_per_epoch=STEP_SIZE_VALID,
        #   validation_data=validation_generator,
        #   validation_steps=STEP_SIZE_VALID,
          epochs=2, shuffle=True,
          use_multiprocessing=False, verbose=1)

# Sprawdzenie poprawności na danych testowych po trenowaniu na zbiorze walidacyjnym.
test_generator.reset()
STEP_SIZE_TEST = test_generator.n // test_generator.batch_size 
scores = model.evaluate(test_generator, 
                        steps=STEP_SIZE_TEST,
                        verbose=0)
print(model.metrics_names[0] + '=', scores[0])
print(model.metrics_names[1] + '=', scores[1])

test_generator.reset()
probabilities = model.predict(test_generator, 
                              steps=STEP_SIZE_TEST,
                              verbose=0)
y_test = probabilities.argmax(axis=1)
y_pred = test_generator.classes
target_names = sorted(os.listdir(dir_path + "/test"))
print(classification_report(y_test, y_pred, target_names=target_names))

# Zapisanie pliku .csv porównując typ i predykcję, jest podane zdjęcie więc łatwo będzie sprawdzić co z nim nie tak
test_generator.reset()
probabilities = model.predict(test_generator, 
                              steps=STEP_SIZE_TEST,
                              verbose=1)
predicted_class_indices = np.argmax(probabilities, axis=1)
labels = (train_generator.class_indices)
labels = dict((v, k) for k, v in labels.items())
predictions = [labels[k] for k in predicted_class_indices]

filenames = test_generator.filenames
results=pd.DataFrame({"Filename": filenames,
                      "Predictions": predictions,})
results.to_csv("results.csv", sep=';', index=False)

# Tworzenie schematu zawierającego kształt warstw.
from keras.utils import plot_model
plot_model(base_model, show_shapes=True, to_file="model.png")